\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduzione}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Cos'è il Topic Modeling}{5}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Stato dell'arte}{5}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Obiettivi e motivazioni}{6}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Novità nella letteratura}{6}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Metodologia}{6}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Risultati}{7}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Conclusioni}{8}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Rilevanza per questo progetto}{8}{subsection.1.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}BERTopic}{10}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Funzionalità}{11}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Embedding}{11}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Esempio illustrativo di relazioni semantiche nello spazio degli embedding: differenze vettoriali come $E(\text  {man})-E(\text  {woman})$ codificano attributi latenti (qui il genere).}}{12}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:embedding-analogy}{{2.1}{12}{Esempio illustrativo di relazioni semantiche nello spazio degli embedding: differenze vettoriali come $E(\text {man})-E(\text {woman})$ codificano attributi latenti (qui il genere)}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Dimensionality Reduction}{13}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fuzzy simplicial complex}{13}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Proiezione UMAP nel piano bidimensionale. Dopo l'intersezione con il primo \emph  {neighbour}, il cerchio diventa \emph  {sfumato}, riducendo il peso della connessione.\footnotemark }}{14}{figure.caption.4}\protected@file@percent }
\newlabel{fig:umap-projection}{{2.2}{14}{Proiezione UMAP nel piano bidimensionale. Dopo l'intersezione con il primo \emph {neighbour}, il cerchio diventa \emph {sfumato}, riducendo il peso della connessione.\protect \footnotemark }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Parametri}{14}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\texttt  {n\_neighbors}.}{15}{section*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Proiezione UMAP di un modello 3D con 50\,000 punti in un piano 2D. Fonti: \href  {https://3d.si.edu/object/3d/mammuthus-primigenius-blumbach:341c96cd-f967-4540-8ed1-d3fc56d31f12}{Smithsonian 3D Digitization Program} e \href  {https://www.maxnoichl.eu/projects/mammoth/}{Max Noichl}.}}{15}{figure.caption.7}\protected@file@percent }
\newlabel{fig:umap-mammoth}{{2.3}{15}{Proiezione UMAP di un modello 3D con 50\,000 punti in un piano 2D. Fonti: \href {https://3d.si.edu/object/3d/mammuthus-primigenius-blumbach:341c96cd-f967-4540-8ed1-d3fc56d31f12}{Smithsonian 3D Digitization Program} e \href {https://www.maxnoichl.eu/projects/mammoth/}{Max Noichl}}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{\texttt  {min\_dist}}{16}{section*.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Effetto di \texttt  {min\_dist} sulla proiezione UMAP del modello 3D da 50\,000 punti. Fonti: \href  {https://3d.si.edu/object/3d/mammuthus-primigenius-blumbach:341c96cd-f967-4540-8ed1-d3fc56d31f12}{Smithsonian 3D Digitization Program} e \href  {https://www.maxnoichl.eu/projects/mammoth/}{Max Noichl}.}}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:umap-min-dist}{{2.4}{16}{Effetto di \texttt {min\_dist} sulla proiezione UMAP del modello 3D da 50\,000 punti. Fonti: \href {https://3d.si.edu/object/3d/mammuthus-primigenius-blumbach:341c96cd-f967-4540-8ed1-d3fc56d31f12}{Smithsonian 3D Digitization Program} e \href {https://www.maxnoichl.eu/projects/mammoth/}{Max Noichl}}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Clustering}{16}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Dataset iniziale \textit  {ad hoc} generato per testare HDBSCAN}}{17}{figure.caption.10}\protected@file@percent }
\newlabel{fig:hdbscan-points}{{2.5}{17}{Dataset iniziale \textit {ad hoc} generato per testare HDBSCAN}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Minimum Spanning Tree costruito sulle distanze di mutual reachability: gli archi a peso minore collegano regioni dense e fungono da base per la gerarchia.}}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig:hdbscan-mst}{{2.6}{19}{Minimum Spanning Tree costruito sulle distanze di mutual reachability: gli archi a peso minore collegano regioni dense e fungono da base per la gerarchia}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Esempio di gerarchia dei cluster derivata dalla rimozione progressiva degli archi del MST: i nodi mostrano come i gruppi nascano e si separino a densità crescenti.}}{21}{figure.caption.15}\protected@file@percent }
\newlabel{fig:hdbscan-cluster-hierarchy}{{2.7}{21}{Esempio di gerarchia dei cluster derivata dalla rimozione progressiva degli archi del MST: i nodi mostrano come i gruppi nascano e si separino a densità crescenti}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Albero condensato generato applicando \texttt  {min\_cluster\_size}: vengono eliminati i rami instabili e restano solo i cluster con sufficiente supporto.}}{22}{figure.caption.17}\protected@file@percent }
\newlabel{fig:hdbscan-condensed}{{2.8}{22}{Albero condensato generato applicando \texttt {min\_cluster\_size}: vengono eliminati i rami instabili e restano solo i cluster con sufficiente supporto}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Cluster finali selezionati in base alla stabilità: i colori identificano le componenti persistenti, mentre i punti grigi rappresentano il rumore (label $-1$).}}{23}{figure.caption.19}\protected@file@percent }
\newlabel{fig:hdbscan-final-clusters}{{2.9}{23}{Cluster finali selezionati in base alla stabilità: i colori identificano le componenti persistenti, mentre i punti grigi rappresentano il rumore (label $-1$)}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}CountVectorizer}{23}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}c-TF-IDF}{24}{subsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Come viene usato il peso}{25}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Maximal Marginal Relevance (MMR)}{25}{subsection.2.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Implementazione di BERTopic}{27}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Embedding}{27}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Statistiche globali del dataset (totale annunci: 5357).}}{28}{figure.caption.21}\protected@file@percent }
\newlabel{fig:dataset-stats}{{3.1}{28}{Statistiche globali del dataset (totale annunci: 5357)}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Mean-max pooling}{29}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Dimensionality Reduction}{30}{section.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Sintesi degli esperimenti al variare di $n\_neighbors$: numero di topic trovati, dimensione media e deviazione standard dei cluster, oltre al conteggio dei topic piccoli ($<100$ documenti), di quelli molto grandi ($>500$ documenti) e al totale di documenti assegnati.}}{31}{table.caption.22}\protected@file@percent }
\newlabel{tab:n-neighbors-summary}{{3.1}{31}{Sintesi degli esperimenti al variare di $n\_neighbors$: numero di topic trovati, dimensione media e deviazione standard dei cluster, oltre al conteggio dei topic piccoli ($<100$ documenti), di quelli molto grandi ($>500$ documenti) e al totale di documenti assegnati}{table.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Numero di topic generati da HDBSCAN al variare di $n\_neighbors$: la scelta intermedia ($n\_neighbors=50$) massimizza la granularità senza collassare in pochi cluster.}}{32}{figure.caption.23}\protected@file@percent }
\newlabel{fig:num-topics-per-neighbors}{{3.2}{32}{Numero di topic generati da HDBSCAN al variare di $n\_neighbors$: la scelta intermedia ($n\_neighbors=50$) massimizza la granularità senza collassare in pochi cluster}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Proiezioni generate da \texttt  {BERTopic.\allowbreak  visualize\_documents()}: i punti (embedding ridotti a 2 dimensioni) sono colorati in base al topic. Confronto tra \texttt  {n\_neighbors} pari a 15, 50 e 100.}}{33}{figure.caption.24}\protected@file@percent }
\newlabel{fig:umap-neighbors-comparison}{{3.3}{33}{Proiezioni generate da \texttt {BERTopic.\allowbreak visualize\_documents()}: i punti (embedding ridotti a 2 dimensioni) sono colorati in base al topic. Confronto tra \texttt {n\_neighbors} pari a 15, 50 e 100}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Clustering}{33}{section.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Metriche chiave al variare di \texttt  {min\_samples}: numero di topic (n\_topics), percentuale di rumore (noise), statistiche di dimensione dei cluster, quota del topic principale e conteggio degli outlier.}}{34}{table.caption.25}\protected@file@percent }
\newlabel{tab:min-samples-summary}{{3.2}{34}{Metriche chiave al variare di \texttt {min\_samples}: numero di topic (n\_topics), percentuale di rumore (noise), statistiche di dimensione dei cluster, quota del topic principale e conteggio degli outlier}{table.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Effetto di \texttt  {min\_samples} sulla distribuzione dei topic: visualizzazioni prodotte da \texttt  {BERTopic.visualize\_documents()} in configurazioni crescenti del parametro.}}{35}{figure.caption.26}\protected@file@percent }
\newlabel{fig:min-samples-umap}{{3.4}{35}{Effetto di \texttt {min\_samples} sulla distribuzione dei topic: visualizzazioni prodotte da \texttt {BERTopic.visualize\_documents()} in configurazioni crescenti del parametro}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}CountVectorizer}{35}{subsection.3.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Principali parametri del \texttt  {CountVectorizer}.}}{36}{table.caption.27}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Data Cleaning}{38}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Perché il Data Cleaning è necessario}{38}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Barplot ottenuto da \textit  {topic modeling} senza \textit  {preprocess}.}}{39}{figure.caption.28}\protected@file@percent }
\newlabel{fig:garbage-barplot}{{4.1}{39}{Barplot ottenuto da \textit {topic modeling} senza \textit {preprocess}}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Esempio annotato di annuncio di lavoro e sezioni considerate nella fase di data cleaning.}}{40}{figure.caption.29}\protected@file@percent }
\newlabel{fig:job-offer-example}{{4.2}{40}{Esempio annotato di annuncio di lavoro e sezioni considerate nella fase di data cleaning}{figure.caption.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Divisione in paragrafi}{41}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Riconoscere le intestazioni}{42}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Spacy}{42}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Suggester}{44}{section*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Configurazione del \textit  {suggester}: usa il componente \texttt  {spacy.ngram\_suggester.v1} e valuta tutti gli span fino a 20 token per coprire la varietà degli header.}}{44}{figure.caption.32}\protected@file@percent }
\newlabel{fig:cfg-suggester}{{4.3}{44}{Configurazione del \textit {suggester}: usa il componente \texttt {spacy.ngram\_suggester.v1} e valuta tutti gli span fino a 20 token per coprire la varietà degli header}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tok2Vec}{44}{section*.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Configurazione dell'\textit  {embedder}: vettori da 96 dimensioni con attributi morfologici distinti (\texttt  {NORM}, \texttt  {PREFIX}, \texttt  {SUFFIX}, \texttt  {SHAPE}) e hash table calibrate su 5000/1000/2500/2500 voci; gli embedding esterni non vengono usati.}}{45}{figure.caption.34}\protected@file@percent }
\newlabel{fig:cfg-embed}{{4.4}{45}{Configurazione dell'\textit {embedder}: vettori da 96 dimensioni con attributi morfologici distinti (\texttt {NORM}, \texttt {PREFIX}, \texttt {SUFFIX}, \texttt {SHAPE}) e hash table calibrate su 5000/1000/2500/2500 voci; gli embedding esterni non vengono usati}{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Encoder \texttt  {MaxoutWindowEncoder}: stesso spazio vettoriale a 96 dimensioni, quattro strati con finestra contestuale di un token per lato e tre proiezioni Maxout per modellare non linearità.}}{46}{figure.caption.35}\protected@file@percent }
\newlabel{fig:cfg-encode}{{4.5}{46}{Encoder \texttt {MaxoutWindowEncoder}: stesso spazio vettoriale a 96 dimensioni, quattro strati con finestra contestuale di un token per lato e tre proiezioni Maxout per modellare non linearità}{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{Reducer}{46}{section*.36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Reducer \texttt  {mean\_max}: combina media e massimo dei token in un vettore di dimensione 128 prima dello strato di scoring.}}{46}{figure.caption.37}\protected@file@percent }
\newlabel{fig:cfg-reducer}{{4.6}{46}{Reducer \texttt {mean\_max}: combina media e massimo dei token in un vettore di dimensione 128 prima dello strato di scoring}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Scorer}{47}{section*.38}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Scorer logistico lineare: un'unica uscita sigmoide (\texttt  {nO = 1}) alimentata dal vettore di dimensione 128 prodotto dal reducer.}}{47}{figure.caption.39}\protected@file@percent }
\newlabel{fig:cfg-scorer}{{4.7}{47}{Scorer logistico lineare: un'unica uscita sigmoide (\texttt {nO = 1}) alimentata dal vettore di dimensione 128 prodotto dal reducer}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsubsection}{Allenamento}{47}{section*.40}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Parametri di training: dropout fissato a 0.1, aggiornamento dopo ogni batch (\texttt  {accumulate\_gradient = 1}), early stopping con \texttt  {patience} 1600 valutazioni e \texttt  {max\_steps} 20000; valutazione sul dev ogni 200 step.}}{48}{figure.caption.41}\protected@file@percent }
\newlabel{fig:cfg-training}{{4.8}{48}{Parametri di training: dropout fissato a 0.1, aggiornamento dopo ogni batch (\texttt {accumulate\_gradient = 1}), early stopping con \texttt {patience} 1600 valutazioni e \texttt {max\_steps} 20000; valutazione sul dev ogni 200 step}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Valutazione del modello}{49}{subsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Confronto delle metriche \textit  {precision}, \textit  {recall} e \textit  {F-score} tra la valutazione fatta nel dev set e quella nel test set.}}{49}{figure.caption.42}\protected@file@percent }
\newlabel{fig:header-metrics}{{4.9}{49}{Confronto delle metriche \textit {precision}, \textit {recall} e \textit {F-score} tra la valutazione fatta nel dev set e quella nel test set}{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Fallback}{49}{subsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Funzione di fallback: sfrutta i token e il modulo \texttt  {sentencizer} di spaCy (all'interno di \texttt  {count\_sentences}) per riconoscere le frasi in un blocco di testo.}}{50}{figure.caption.43}\protected@file@percent }
\newlabel{fig:fallback-split}{{4.10}{50}{Funzione di fallback: sfrutta i token e il modulo \texttt {sentencizer} di spaCy (all'interno di \texttt {count\_sentences}) per riconoscere le frasi in un blocco di testo}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Classificazione paragrafi}{51}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Etichettatura}{51}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Allenamento}{54}{section*.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Il 20\% del dataset è usato come test set per la valutazione. L'opzione \texttt  {stratify=df["label"]} preserva la distribuzione delle classi tra train e test, indispensabile in presenza di classi sbilanciate.}}{54}{figure.caption.45}\protected@file@percent }
\newlabel{fig:clf-split}{{4.11}{54}{Il 20\% del dataset è usato come test set per la valutazione. L'opzione \texttt {stratify=df["label"]} preserva la distribuzione delle classi tra train e test, indispensabile in presenza di classi sbilanciate}{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Configurazione del classificatore LR: massimo 2000 iterazioni, pesi maggiorati per la classe \texttt  {JOB} (1.5) e bilanciamento minore per \texttt  {BLURB} e \texttt  {DETAIL}.}}{55}{figure.caption.46}\protected@file@percent }
\newlabel{fig:clf-lr}{{4.12}{55}{Configurazione del classificatore LR: massimo 2000 iterazioni, pesi maggiorati per la classe \texttt {JOB} (1.5) e bilanciamento minore per \texttt {BLURB} e \texttt {DETAIL}}{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Metriche di valutazione e matrici di confusione al variare del peso assegnato alla classe \textbf  {JOB} nella \textit  {Logistic Regression}.}}{56}{figure.caption.47}\protected@file@percent }
\newlabel{fig:lr-reports}{{4.13}{56}{Metriche di valutazione e matrici di confusione al variare del peso assegnato alla classe \textbf {JOB} nella \textit {Logistic Regression}}{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsubsection}{Strategia di rimozione}{56}{section*.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Logica di post-processing per l'assegnazione finale della classe del paragrafo.}}{57}{figure.caption.49}\protected@file@percent }
\newlabel{fig:lr-postprocess}{{4.14}{57}{Logica di post-processing per l'assegnazione finale della classe del paragrafo}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Altre strategie di data cleaning usate}{57}{section.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Dendrogramma di BERTopic su dataset parzialmente pulito.}}{58}{figure.caption.50}\protected@file@percent }
\newlabel{fig:half-cleaning}{{4.15}{58}{Dendrogramma di BERTopic su dataset parzialmente pulito}{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pulizia tramite NER}{58}{section*.51}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Euristica di rimozione delle entità: \texttt  {ORG} identifica le organizzazioni (incluse le aziende), \texttt  {GPE} le entità geopolitiche (stati, città).}}{59}{figure.caption.52}\protected@file@percent }
\newlabel{fig:ner-cleaning}{{4.16}{59}{Euristica di rimozione delle entità: \texttt {ORG} identifica le organizzazioni (incluse le aziende), \texttt {GPE} le entità geopolitiche (stati, città)}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsubsection}{Risultato finale}{59}{section*.53}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Dendrogramma di BERTopic su dataset completamente pulito.}}{59}{figure.caption.54}\protected@file@percent }
\newlabel{fig:cleaned-dendrogram}{{4.17}{59}{Dendrogramma di BERTopic su dataset completamente pulito}{figure.caption.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Possibili miglioramenti}{60}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Risultati finali}{61}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{*}
\bibstyle{unsrt}
\bibdata{bibliografia}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusioni}{62}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{almgerbi2025dynamics}{1}
\bibcite{grootendorst2022bertopic}{2}
\bibcite{devlin2019bert}{3}
\@writefile{toc}{\contentsline {chapter}{Ringraziamenti}{64}{chapter*.56}\protected@file@percent }
\gdef \@abspage@last{64}
