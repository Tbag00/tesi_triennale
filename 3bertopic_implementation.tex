\chapter{Implementazione di BERTopic}

\section{Embedding}
I modelli principali consigliati nella documentazione\footnote{\url{https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html}} sono i \textbf{sentence transformers} (SBERT).
Ci sono molti modelli SBERT pre-addestrati tra cui scegliere, la documentazione ufficiale nei suoi esempi usa spesso all-MiniLM-L6-v2 un modello estremamente leggero e veloce, però non sufficientemente preciso nel catturare sfumature semantiche complesse o relazioni a lungo raggio.
paraphrase-MiniLM-L12-v2 e paraphrase-mpnet-base-v2, sono più accurati, ma più specializzati nel catturare relazioni di parafrasi, fra testi molto simili, sono inoltre inadatti a testi particolarmente lunghi come gli annunci di lavoro.
\textbf{all-mpnet-base-v2}, invece, ha una alta \textbf{precisione semantica}, e si comporta bene con documenti di lunghezza \textbf{medio-lunga}.
Ha però un limite di lunghezza di \textbf{512 token}.
Ecco alcune informazioni sulla lunghezza del nostro dataset (dopo la pulizia):
\begin{figure}[H]
\centering
\scriptsize
\begin{tabular}{lccc}
\hline
 & n chars & n words & n tokens mpnet \\
\hline
count & 5357.00 & 5357.00 & 5357.00 \\
mean & 3315.48 & 457.92 & 594.54 \\
std & 1591.28 & 219.01 & 283.37 \\
min & 91.00 & 14.00 & 23.00 \\
50\% & 3087.00 & 427.00 & 554.00 \\
75\% & 4060.00 & 562.00 & 729.00 \\
90\% & 5213.40 & 717.00 & 934.00 \\
95\% & 6188.20 & 846.20 & 1103.40 \\
99\% & 8797.52 & 1203.44 & 1562.72 \\
max & 19470.00 & 2739.00 & 3753.00 \\
\hline
\multicolumn{4}{l}{Testi che superano i 512 token: 3075 (57.40\%)} \\
\multicolumn{4}{l}{Totale annunci: 5357} \\
\hline
\end{tabular}
\caption{Statistiche globali del dataset (totale annunci: 5357).}
\label{fig:dataset-stats}
\end{figure}
\noindent Quando SBERT riceve un input troppo lungo, lo \textbf{tronca} questo significa che applicando l'embedder così come è verrebbe tagliata una porzione molto grande del dataset.

\noindent Le prime opzioni che abbiamo considerato sono:
\begin{enumerate}
    \item Affidarsi ad un modello con un limite di dimensione input più alto (ad esempio \textbf{all-roberta-long-v1})
    \item Frammentare le descrizioni e fare il topic modeling in segmenti di quest'ultime.
\end{enumerate}
Il vantaggio della prima opzione è che i modelli \textit{long-context}, mantengono attenzione tra termini distanti, permettendo di cogliere relazioni semantiche a lungo raggio, cosa impossibile se si divide il testo in \textit{chunk}.
Non è però una caratteristica che abbiamo ritenuto significativa, data la natura del nostro \textit{dataset}, infatti i nostri paragrafi hanno natura scollegata, uno potrebbe parlare di competenze e un altro di mansioni, la coerenza globale è più importante in testi di natura \textbf{discorsiva}.
Inoltre se la lunghezza del testo è grande è probabile che contenga più temi, un unico embedding rischia di mescolare \textbf{sezioni semanticamente diverse}, ottenendo topic più grossolani.
La seconda opzione invece, oltre che generare topic più \textbf{puliti} e \textbf{interpretabili}, consente di aumentare molto il numero di documenti in input e questo è un vantaggio per dataset esegui come il nostro.
Però anche questo caso presenta delle criticità: non avremmo i topic relativi agli annunci come uniche entità, ma saranno relativi ai segmenti e andrebbero aggregati a posteriori per ottenere una \textbf{distribuzione di topic per annuncio}. In più in questo modo, gli annunci più lunghi avranno \textbf{più peso}, semplicemente perché hanno più paragrafi. La situazione si complica ulteriormente se i documenti contengono paragrafi simili, questo creerebbe dei \textbf{cluster} artefatti e quindi topic che non rispecchiano la natura del dataset. Un altro problema è che con questo metodo il modello è completamente ignaro della struttura globale dell'annuncio.
Questo approccio è più attuabile per risolvere un problema di \textbf{classificazione}, non per il \textit{topic modeling}.\medskip

\noindent La soluzione che abbiamo scelto è dunque quella di \textit{mean-max-pooling}.
\subsection{Mean-max pooling}

\noindent Con questa tecnica si ottiene un embedding per ogni annuncio partendo dagli embedding dei paragrafi, seguendo i passaggi seguenti:
\begin{enumerate}
    \item calcolare gli embedding di ciascun paragrafo;
    \item normalizzare ogni embedding;
    \item calcolare media aritmetica e massimo elemento per elemento;
    \item concatenare i due vettori risultanti;
    \item normalizzare nuovamente il vettore concatenato.
\end{enumerate}

In questo modo si ottiene un embedding unico di dimensione doppia rispetto l'output dell'embedder che può catturare sia il \textbf{significato generale} dell'annuncio, sia le \textbf{feature salienti} dei singoli paragrafi: queste due componenti possono produrre vettori più \textbf{discriminativi} e quindi più facilmente separabili nello spazio semantico.

Per attuare questo approccio serve inoltre una strategia di segmentazione degli annunci che soddisfi al contempo \textbf{coerenza semantica} e criteri di lunghezza: il segmento non può essere troppo corto, altrimenti SBERT non ha il contesto per estrarre topic singnificativi, e non può ovviamente superare la lunghezza massima consentita. Approfondiamo questo aspetto nel capitolo \textbf{Pre-processing}.

\subsection*{Tuning congiunto di UMAP e HDBSCAN}
Poiché sia il parametro \texttt{n\_neighbors} di UMAP che il parametro \texttt{min\_samples} di HDBSCAN influiscono pesantemente sul bilanciamento tra struttura \textbf{locale} e \textbf{globale}, abbiamo deciso di testarli combinandoli in più modi:
\[
n\_neighbors \in \{15, 25, 35, 50\}, \qquad
min\_samples \in \{1, 5, 10, 15, 20, 25, 30\}.
\]
L'obiettivo era individuare la combinazione che producesse il miglior equilibrio tra:
\begin{enumerate}
    \item numero di cluster generati (\emph{n\_clusters}), indice della granularità semantica del modello;
    \item percentuale di punti marcati come rumore (\emph{noise\_\%}), legata alla severità della densità richiesta da HDBSCAN;
    \item probabilità media di appartenenza ai topic (\emph{mean\_topic\_prob}), che misura la coerenza semantica interna dei cluster.
\end{enumerate}

Per riproducibilità segnaliamo anche gli altri iperparametri dei due modelli (Figura~\ref{fig:umap-hdbscan-config}): l’impostazione \texttt{min\_dist = 0.1} mantiene gli embedding sufficientemente separati per generare cluster ben delineati senza il collasso delle regioni a bassa densità.

\begin{figure}[H]
\centering
\begin{lstlisting}[language=Python]
umap_model = UMAP(
    n_components=10,
    min_dist=0.1,
    metric='euclidean',
    random_state=1
)
hdbscan_model = HDBSCAN(
    min_cluster_size=25,
    metric='euclidean',
    cluster_selection_method='eom',
    prediction_data=True,
    cluster_selection_epsilon=0
)
\end{lstlisting}
\caption{Configurazione utilizzata per UMAP e HDBSCAN durante il tuning intermedio. L’impostazione \texttt{min\_dist = 0.1} preserva la separabilità degli embedding evitando il collasso osservato con valori più bassi.}
\label{fig:umap-hdbscan-config}
\end{figure}

\noindent
I risultati principali sono riassunti nella Tabella~\ref{tab:tuning_umap_hdbscan}.

\begin{table}[H]
\centering
\caption{Risultati del tuning congiunto UMAP–HDBSCAN. Rimosse alcune righe per brevità}
\label{tab:tuning_umap_hdbscan}
\begin{tabular}{ccccc}
\textbf{n\_neighbors} & \textbf{min\_samples} & \textbf{n\_clusters} & \textbf{noise [\%]} & \textbf{mean\_topic\_prob} \\
15 & 1  & 47 & 40.23 & 0.560 \\
15 & 10 & 27 & 58.05 & 0.527 \\
15 & 15 & 22 & 53.89 & 0.483 \\
25 & 5  & 2  & 2.00  & 0.886 \\
25 & 10 & 25 & 48.37 & 0.469 \\
25 & 20 & 2  & 5.81  & 0.826 \\
35 & 1  & 43 & 38.55 & 0.543 \\
35 & 10 & 22 & 49.65 & 0.466 \\
35 & 15 & 7  & 22.77 & 0.538 \\
50 & 10 & 21 & 56.28 & 0.482 \\
50 & 15 & 14 & 52.04 & 0.469 \\
\end{tabular}
\end{table}

\noindent
Dalla tabella emergono alcune tendenze chiare:
\begin{enumerate}
    \item All'aumentare di \texttt{min\_samples}, il numero di cluster decresce, segno di una maggiore severità nella definizione della densità locale, come previsto dal comportamento teorico di HDBSCAN.
    \item Il rumore cresce progressivamente da circa 40\% fino a 55\%, indicando che i punti periferici vengono espulsi in modo coerente con la maggiore compattezza richiesta dai cluster.
    \item La probabilità media di appartenenza ai topic mostra un andamento decrescente, passando da valori intorno a 0.55 per configurazioni più permissive fino a circa 0.43 per modelli più restrittivi.
\end{enumerate}

\noindent
Le configurazioni con \texttt{n\_neighbors = 25} presentano valori anomali (solo due cluster con \emph{mean\_topic\_prob} superiore a 0.8): in questi casi, UMAP ha compattato eccessivamente lo spazio delle rappresentazioni, portando HDBSCAN a fondere quasi tutti i punti in pochissimi cluster molto densi. Tali risultati, pur mostrando coerenza interna elevata, non sono semanticamente informativi e vengono scartati.

Le configurazioni con \texttt{n\_neighbors = 15} producono un numero elevato di cluster (fino a 47), ma con livelli di rumore superiori al 50\%, sintomo di eccessiva frammentazione.  
Al contrario, \texttt{n\_neighbors = 50} genera cluster più ampi e meno granulari, ma anche un leggero aumento del rumore, dovuto alla perdita di dettaglio nella struttura locale.
Abbiamo dunque deciso di fare un altro tentativo restringendo però il dominio dei valori:

\[
n\_neighbors \in \{30, 35, 40 ,45, 50\}, \qquad
min\_samples \in \{10, 15, 20, 25, 30\}.
\]

\noindent
La Tabella~\ref{tab:tuning_seconda_iterazione} riporta i principali risultati ottenuti.

\begin{table}[H]
\centering
\caption{Risultati della seconda iterazione di tuning congiunto UMAP–HDBSCAN.}
\label{tab:tuning_seconda_iterazione}
\begin{tabular}{ccccc}
\textbf{n\_neighbors} & \textbf{min\_samples} & \textbf{n\_clusters} & \textbf{noise [\%]} & \textbf{mean\_topic\_prob} \\
30 & 10 & 24 & 51.91 & 0.490 \\
30 & 15 & 15 & 49.52 & 0.486 \\
35 & 10 & 22 & 49.65 & 0.466 \\
35 & 15 & 7  & 22.77 & 0.538 \\
40 & 10 & 22 & 49.97 & 0.488 \\
40 & 15 & 19 & 55.03 & 0.474 \\
45 & 15 & 18 & 54.38 & 0.451 \\
50 & 10 & 21 & 56.28 & 0.482 \\
50 & 15 & 14 & 52.04 & 0.469 \\
\end{tabular}
\end{table}

Si osserva come valori di \texttt{n\_neighbors} superiori a 40 tendano a
compattare eccessivamente lo spazio latente, portando HDBSCAN a fondere
i sottotemi in pochi cluster di grandi dimensioni.
Al contrario, valori più bassi (intorno a 30–35) preservano una
struttura locale più dettagliata, permettendo di identificare un numero
maggiore di argomenti distinti con un livello di rumore ancora accettabile.

La configurazione
\[
n\_neighbors = 30, \quad min\_samples = 10
\]
ha prodotto $24$ cluster con una percentuale di rumore del $51.9\%$
e una \emph{mean topic probability} di $0.49$,
rappresentando così il miglior compromesso fra granularità e stabilità.
Questa combinazione consente di ottenere una rappresentazione più articolata
dei contenuti semantici del corpus, mantenendo al contempo una coerenza interna adeguata.

\paragraph{Confronto con la prima iterazione.}
Rispetto alla prima esplorazione, in cui la combinazione
$n\_neighbors = 35$, $min\_samples = 15$ aveva prodotto solo $7$ cluster
ad alta coesione (rumore 22\%),
questa seconda iterazione mostra come una lieve riduzione di
\texttt{min\_samples} e di \texttt{n\_neighbors} permetta di recuperare
una maggiore granularità tematica senza compromettere in modo significativo
la qualità dei cluster.
In altre parole, la nuova configurazione genera topic più numerosi
ma ancora interpretabili e semanticamente coerenti.

\paragraph{Scelta finale.}
Per l’analisi definitiva dei risultati si è quindi adottata la seguente configurazione:
\[
\boxed{
n\_neighbors = 30, \quad min\_samples = 10, \quad min\_dist = 0.1, \quad metric = \text{euclidean}.
}
\]
Questi valori offrono un equilibrio ottimale fra livello di dettaglio dei topic
e stabilità del modello, evitando sia la frammentazione eccessiva,
sia il collasso dei cluster osservato in configurazioni più estreme.

\paragraph{Osservazione finale.}
L’allineamento delle metriche tra UMAP e HDBSCAN (\texttt{euclidean}) e l’introduzione di un valore di \texttt{min\_dist = 0.1}
si sono rivelati determinanti per ottenere risultati stabili e coerenti con la teoria della densità.
Queste modifiche hanno eliminato i fenomeni di ``collasso dello spazio'' osservati nelle run precedenti
(\emph{min\_dist = 0.0}), restituendo una distribuzione più realistica dei punti e un comportamento regolare
del modello di clustering.

\subsection{CountVectorizer}

\noindent La configurazione del vettorizzatore influenza fortemente la qualità dei topic risultanti. 
Tra i principali parametri utilizzabili si distinguono:

\begin{table}[H]
\centering
\begin{tabular}{p{3cm} p{9cm}}
\textbf{Parametro} & \textbf{Descrizione} \\
\texttt{stop\_words} & Elenco di parole da escludere (articoli, preposizioni, ecc.). Può assumere valori come \texttt{'english'} o una lista personalizzata di termini. \\
\texttt{ngram\_range} & Tuple che definisce la lunghezza minima e massima delle n-gram; ad esempio \texttt{(1, 2)} include sia unigrammi che bigrammi, consentendo di catturare espressioni come \textit{data analysis}. \\
\texttt{min\_df} & Numero minimo di documenti (o frazione) in cui un termine deve apparire per essere incluso nel vocabolario; filtra i termini troppo rari. \\
\texttt{max\_df} & Numero massimo di documenti (o frazione) in cui un termine può apparire prima di essere escluso; filtra i termini troppo frequenti o generici. \\
\end{tabular}
\caption{Principali parametri del \texttt{CountVectorizer}.}
\end{table}

\noindent Alle normali \emph{stop words} inglesi abbiamo aggiunto alcune specifiche che rispecchiano il gergo del nostro dataset:

\begin{lstlisting}[language=Python]
custom_stopwords = [
    # anni e numeri
    "2024", "2025", "2026", "19xx", "20xx",

    # frasi burocratiche / pattern ricorrenti
    "regular", "job", "regular job", [...]
    # termini amministrativi o HR generici
    "eligibility", "degree", "foreign", "equivalent",[...]
    # altri pattern burocratici o poco informativi
    "agency", "equipment", "reports", "dashboards", "hoc",
    "solutions",
    # forme sinonimiche o doppioni rumorosi
    "equivalency years", "years", "regular job code", "employment type regular",
    "type regular job", "employment type", "regular job",
    # placeholder testuali
    "institutional", "equivalency", "perform essential",
]
\end{lstlisting}
